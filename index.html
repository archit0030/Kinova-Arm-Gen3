<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Enhanced Robotic Task Automation</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
        }
        header {
            text-align: center;
            margin-bottom: 40px;
        }
        header h1 {
            font-size: 2.5em;
            margin-bottom: 0.2em;
        }
        section {
            margin-bottom: 20px;
        }
        section h2 {
            font-size: 1.8em;
            color: #333;
            margin-bottom: 10px;
        }
        section p {
            margin: 10px 0;
        }
        footer {
            text-align: center;
            margin-top: 40px;
            font-size: 0.9em;
            color: #666;
        }
        .video-container {
            text-align: center;
        }
        .video-container video {
            max-width: 100%;
            height: auto;
        }
    </style>
</head>
<body>

    <header>
        <h1>Enhanced Robotic Task Automation through Detailed Sub-action Recognition and Object-Aware Execution</h1>
        <p>Exploring advancements in robotic automation through smaller language models and object-aware execution</p>
    </header>

    <section>
        <h2>Introduction</h2>
        <p>Large Language Models (LLMs) have demonstrated strong reasoning abilities in robotics, enabling robots to understand user intentions and deduce task workflows effectively. However, applying LLMs to intricate robotic tasks presents challenges. While LLMs can generate detailed steps, these steps are often not directly executable without the integration of operational knowledge and skill. Many current methods for robotic task execution and text-to-action mapping rely on GPT models, which have significant dependencies. These dependencies include the need for internet access to use the latest GPT-4 model and paid subscriptions. In contrast, our work focuses on creating a customized, smaller language model using pre-trained models like BERT. This model is designed to efficiently extract key sub-actions from human commands and carry out the corresponding actions on robots without relying on larger models with heavy external dependencies. Our work focuses on recognizing smaller actions within larger activities, rather than just identifying complete actions or tasks. We aim to understand and execute these smaller actions, which can then be combined to create entirely new actions. This unique approach enhances the flexibility and adaptability of robotic systems, allowing them to perform tasks that were not explicitly pre-programmed. In addition to recognizing sub-actions, our approach also considers object-aware execution, where each sub-action is associated with specific objects. This allows us to determine not only what action to perform but also which object to manipulate and where to place it. In our paper, we propose a new framework that can identify sub-actions within a larger action using two different methods.  The first method involves extracting sub-actions from spoken instructions using voice commands and the Google API. Finally, the second method uses a sub-action extraction from voice Command using LSTM on a custom dataset, to identify sub-actions within a given action.

Identifying sub-actions, along with their associated objects, not only helps us better understand tasks but also enables the execution of these sub-actions using a robotic manipulator. By constructing new actions from sets of identified sub-actions, our approach expands the range of tasks that can be autonomously performed, representing a significant advancement in the field of robotic task execution. Our paper presents the methods, models, and experimental results demonstrating the effectiveness and versatility of our approach.</p>
    </section>

    <!-- YouTube Video Section -->
    <div class="video-container">
        <h2>Demonstration Video</h2>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/kF7Yb7aYDKs?si=ELU-muSmt7oBDBrG&amp;start=5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        <p>Watch this video to see the robotic task automation in action.</p>
    </div>

    <!-- GitHub-hosted Video Section -->
    <div class="video-container">
        <h2>GitHub-Hosted Video Demonstration</h2>
        <video width="600" controls>
            <source src="https://github.com/archit0030/SubActionRobot/blob/main/videos/Pick%20and%20Stack.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        
        <p>Watch this video for a GitHub-hosted demonstration of the robotic task automation.</p>
    </div>

    <footer>
        <p>&copy; 2024 Enhanced Robotic Task Automation | Powered by GitHub Pages</p>
    </footer>

</body>
</html>
